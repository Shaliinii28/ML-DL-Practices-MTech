{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: unzip dataset.zip file\n",
        "\n",
        "!unzip dataset.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3b7pSGanBS-",
        "outputId": "acc723f7-c76e-45ec-ebc2-ab8845515512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "replace dataset/lemon/1439-2.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: dataset/lemon/1439-2.jpg  \n",
            "replace dataset/lemon/30001186-3_1-fresho-lemon.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace dataset/lemon/659-07028120en_Masterfile.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "u0k0tOUToOk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "labels = []\n",
        "basepath = \"./dataset\"\n",
        "for fobj in os.scandir(basepath):\n",
        "    if fobj.is_dir():\n",
        "      print(fobj.name)\n",
        "      for img in os.scandir(fobj.path):\n",
        "        print(img.path)\n",
        "        x = cv2.imread(img.path)\n",
        "        x = cv2.resize(x, (100, 100))\n",
        "        data.append(x)\n",
        "        labels.append(fobj.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkstzeTVmgYh",
        "outputId": "7c1a8c70-2084-48a4-be5c-398c44e1efc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemon\n",
            "./dataset/lemon/index5.jpg\n",
            "./dataset/lemon/30001186-3_1-fresho-lemon.jpg\n",
            "./dataset/lemon/index7.jpg\n",
            "./dataset/lemon/index2.jpg\n",
            "./dataset/lemon/index1.jpg\n",
            "./dataset/lemon/659-07028120en_Masterfile.jpg\n",
            "./dataset/lemon/index9.jpg\n",
            "./dataset/lemon/index.jpg\n",
            "./dataset/lemon/1439-2.jpg\n",
            "./dataset/lemon/index3.jpg\n",
            "./dataset/lemon/index8.jpg\n",
            "./dataset/lemon/IMG_20191213_200904.jpg\n",
            "./dataset/lemon/IMG_20191213_200917.jpg\n",
            "./dataset/lemon/IMG_20191213_200834.jpg\n",
            "./dataset/lemon/IMG_20191213_200820.jpg\n",
            "./dataset/lemon/IMG_20191213_200848.jpg\n",
            "./dataset/lemon/index4.jpg\n",
            "./dataset/lemon/74-2.jpg\n",
            "./dataset/lemon/index10.jpg\n",
            "./dataset/lemon/p40e65d46fb6e4686b5e388f58e105a19.jpg\n",
            "./dataset/lemon/71whVfSrUdL._SR500,500_.jpg\n",
            "./dataset/lemon/index6.jpg\n",
            "melon\n",
            "./dataset/melon/6.jpg\n",
            "./dataset/melon/9.jpg\n",
            "./dataset/melon/IMG_20191213_201045.jpg\n",
            "./dataset/melon/IMG_20191213_201056.jpg\n",
            "./dataset/melon/5.jpg\n",
            "./dataset/melon/10.jpg\n",
            "./dataset/melon/IMG_20191213_201115.jpg\n",
            "./dataset/melon/4.jpg\n",
            "./dataset/melon/71ogcdh7YjL._SX425_.jpg\n",
            "./dataset/melon/2.jpg\n",
            "./dataset/melon/istockphoto-823296616-612x612.jpg\n",
            "./dataset/melon/1.jpg\n",
            "./dataset/melon/Sensei.jpg\n",
            "./dataset/melon/IMG_20191213_201035.jpg\n",
            "./dataset/melon/2_1212685_e.jpg\n",
            "./dataset/melon/3.jpg\n",
            "./dataset/melon/IMG_20191213_201105.jpg\n",
            "./dataset/melon/7.jpg\n",
            "./dataset/melon/Watermelon.jpg\n",
            "./dataset/melon/8.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.shape(data))\n",
        "print(np.shape(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iubBgU0oJRV",
        "outputId": "9663f738-9cfe-401d-dcdc-20cd320cb603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42, 100, 100, 3)\n",
            "(42,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "ilabels = le.fit_transform(labels)"
      ],
      "metadata": {
        "id": "LITEAVhnpZxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load images from dataset using tensorflow load image from directory\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "IMG_WIDTH = 100\n",
        "IMG_HEIGHT = 100\n",
        "image_size = (IMG_WIDTH, IMG_HEIGHT)\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"dataset\",\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=image_size,\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Configure dataset for performance\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=BUFFER_SIZE)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXZuqNq2o_s6",
        "outputId": "396112a5-2b53-43d8-a433-38fb36a97d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 42 files belonging to 2 classes.\n",
            "(42,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.cardinality().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl7-RwoHqbQs",
        "outputId": "ee17f835-d0ae-4f0f-917d-3c90a811de13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an instance of ImageDataGenerator with augmentation settings\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,            # Normalize pixel values to [0,1]\n",
        "    rotation_range=40,         # Randomly rotate images up to 40 degrees\n",
        "    width_shift_range=0.2,     # Randomly shift images horizontally by 20%\n",
        "    height_shift_range=0.2,    # Randomly shift images vertically by 20%\n",
        "    shear_range=0.2,           # Shear intensity (shear angle in counter-clockwise direction)\n",
        "    zoom_range=0.2,            # Randomly zoom inside pictures\n",
        "    horizontal_flip=True,      # Randomly flip images horizontally\n",
        "    fill_mode='nearest'        # Fill in newly created pixels after a transformation\n",
        ")\n",
        "\n",
        "# Specify the directory where your training images are stored.\n",
        "# The directory should have one subdirectory per class.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'dataset',  # Replace with your directory path\n",
        "    target_size=(150, 150),        # Resize images to 150x150 pixels\n",
        "    batch_size=32,\n",
        "    class_mode='binary'            # Use 'binary' for 2 classes, or 'categorical' for more than 2 classes\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL1nMQnLrR8d",
        "outputId": "09ffb8e7-a059-4883-c267-e75bf02f0f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 42 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using the generator to train a model\n",
        "# Assume you have already defined and compiled a Keras model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,  # Adjust based on the size of your dataset\n",
        "    epochs=50\n",
        ")\n"
      ],
      "metadata": {
        "id": "GGbpSp63uMcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Data Preparation\n",
        "# ----------------------------\n",
        "\n",
        "# Create an instance of ImageDataGenerator for the training set with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,            # Normalize pixel values to [0, 1]\n",
        "    rotation_range=40,         # Randomly rotate images up to 40 degrees\n",
        "    width_shift_range=0.2,     # Randomly shift images horizontally by 20%\n",
        "    height_shift_range=0.2,    # Randomly shift images vertically by 20%\n",
        "    shear_range=0.2,           # Shear intensity (shear angle in counter-clockwise direction)\n",
        "    zoom_range=0.2,            # Randomly zoom inside pictures\n",
        "    horizontal_flip=True,      # Randomly flip images horizontally\n",
        "    fill_mode='nearest'        # Fill in newly created pixels after a transformation\n",
        ")\n",
        "\n",
        "# For validation, we only need to rescale the images\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Replace with your actual directory paths\n",
        "train_dir = 'dataset'\n",
        "validation_dir = 'dataset'\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),  # Resize images to 150x150 pixels\n",
        "    batch_size=32,\n",
        "    class_mode='binary'      # 'binary' for two classes, or 'categorical' for more classes\n",
        ")\n",
        "\n",
        "# Flow validation images in batches of 32 using validation_datagen\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Building the CNN Model\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training the Model\n",
        "# ----------------------------\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhfg2KXOuoCp",
        "outputId": "81af8ae5-5cc8-4001-cfcd-d569789212b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 42 images belonging to 2 classes.\n",
            "Found 42 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers"
      ],
      "metadata": {
        "id": "j93_TLlauzBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    # First convolutional layer\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second convolutional layer\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Third convolutional layer\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Fourth convolutional layer\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Flatten the output and add dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # 'sigmoid' activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',              # Suitable for binary classification\n",
        "    optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "xpoHOXatusXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,      # Adjust based on your dataset size (e.g., total_train_images // batch_size)\n",
        "    epochs=30,                # Number of epochs to train\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50       # Adjust based on your dataset size (e.g., total_validation_images // batch_size)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32k172uJu3B1",
        "outputId": "ad5de5c4-5abe-43f8-c491-da55713d626e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m  2/100\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:37\u001b[0m 2s/step - accuracy: 0.5238 - loss: 0.6855"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.5471 - loss: 0.6810 - val_accuracy: 0.5238 - val_loss: 0.6668\n",
            "Epoch 2/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5266 - loss: 0.8434 - val_accuracy: 0.5238 - val_loss: 0.6667\n",
            "Epoch 3/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.5236 - loss: 0.6730 - val_accuracy: 0.5238 - val_loss: 0.6566\n",
            "Epoch 4/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5239 - loss: 0.6576 - val_accuracy: 0.7143 - val_loss: 0.6345\n",
            "Epoch 5/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.4529 - loss: 0.6705 - val_accuracy: 0.5476 - val_loss: 0.6241\n",
            "Epoch 6/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - accuracy: 0.5481 - loss: 0.6329 - val_accuracy: 0.9524 - val_loss: 0.5990\n",
            "Epoch 7/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.5521 - loss: 0.7213 - val_accuracy: 0.6429 - val_loss: 0.5897\n",
            "Epoch 8/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5717 - loss: 0.6000 - val_accuracy: 0.9524 - val_loss: 0.5625\n",
            "Epoch 9/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.8567 - loss: 0.5841 - val_accuracy: 0.5952 - val_loss: 0.5487\n",
            "Epoch 10/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.6444 - loss: 0.5375 - val_accuracy: 1.0000 - val_loss: 0.5109\n",
            "Epoch 11/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9054 - loss: 0.5133 - val_accuracy: 0.5952 - val_loss: 0.5792\n",
            "Epoch 12/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9263 - loss: 0.4801 - val_accuracy: 1.0000 - val_loss: 0.4141\n",
            "Epoch 13/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.4440 - val_accuracy: 0.9762 - val_loss: 0.3682\n",
            "Epoch 14/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.5983 - loss: 0.5265 - val_accuracy: 0.5952 - val_loss: 0.5086\n",
            "Epoch 15/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.6656 - loss: 0.5463 - val_accuracy: 0.9762 - val_loss: 0.3024\n",
            "Epoch 16/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9761 - loss: 0.3889 - val_accuracy: 0.6667 - val_loss: 0.4082\n",
            "Epoch 17/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9017 - loss: 0.3659 - val_accuracy: 1.0000 - val_loss: 0.2449\n",
            "Epoch 18/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - accuracy: 0.9764 - loss: 0.2978 - val_accuracy: 1.0000 - val_loss: 0.2438\n",
            "Epoch 19/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.2602 - val_accuracy: 0.9762 - val_loss: 0.2168\n",
            "Epoch 20/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9761 - loss: 0.2420 - val_accuracy: 1.0000 - val_loss: 0.1900\n",
            "Epoch 21/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.6454 - loss: 0.5270 - val_accuracy: 1.0000 - val_loss: 0.1925\n",
            "Epoch 22/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9764 - loss: 0.2571 - val_accuracy: 1.0000 - val_loss: 0.1394\n",
            "Epoch 23/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.1775 - val_accuracy: 1.0000 - val_loss: 0.1111\n",
            "Epoch 24/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1388 - val_accuracy: 1.0000 - val_loss: 0.0926\n",
            "Epoch 25/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.9529 - loss: 0.1872 - val_accuracy: 1.0000 - val_loss: 0.1351\n",
            "Epoch 26/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.9764 - loss: 0.1521 - val_accuracy: 1.0000 - val_loss: 0.0896\n",
            "Epoch 27/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.9529 - loss: 0.1818 - val_accuracy: 1.0000 - val_loss: 0.0694\n",
            "Epoch 28/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.1008 - val_accuracy: 1.0000 - val_loss: 0.0567\n",
            "Epoch 29/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0858 - val_accuracy: 1.0000 - val_loss: 0.0466\n",
            "Epoch 30/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0868 - val_accuracy: 0.9762 - val_loss: 0.0956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Reset the generator to start from the beginning\n",
        "validation_generator.reset()\n",
        "\n",
        "# Calculate the number of steps as an integer\n",
        "steps = int(np.ceil(validation_generator.n / validation_generator.batch_size))\n",
        "\n",
        "# Predict on the entire validation set\n",
        "predictions = model.predict(validation_generator, steps=steps)\n",
        "\n",
        "# For binary classification, convert predictions to class labels (0 or 1)\n",
        "predicted_classes = (predictions > 0.5).astype(\"int32\").reshape(-1)\n",
        "\n",
        "# Retrieve the true labels from the generator\n",
        "true_classes = validation_generator.classes\n",
        "\n",
        "# Get the class labels (the keys are the class names)\n",
        "class_labels = list(validation_generator.class_indices.keys())\n",
        "\n",
        "# Generate and print the classification report\n",
        "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlKqxUNkv7ic",
        "outputId": "e84a2022-4c04-4c6d-8f07-01ab20d88f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       lemon       0.52      0.55      0.53        22\n",
            "       melon       0.47      0.45      0.46        20\n",
            "\n",
            "    accuracy                           0.50        42\n",
            "   macro avg       0.50      0.50      0.50        42\n",
            "weighted avg       0.50      0.50      0.50        42\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemon melon dataset\n",
        "data augmentation for the datset\n",
        "CNN model\n",
        "3x3 filter\n",
        "5x5 filter\n",
        "7x7 filter\n",
        "functional model\n",
        "Dnn layers\n",
        "drop out\n",
        "stride padding\n"
      ],
      "metadata": {
        "id": "wyRvA7hfYVih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}